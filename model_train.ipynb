{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importing dependenceis\n",
    "    - Outside packages\n",
    "    - Custom classes\n",
    "\"\"\"\n",
    "\n",
    "# Outside packages\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "import albumentations as A\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# Custom classes\n",
    "sys.path.append('./modules')\n",
    "from datatools1 import make_dataset, load_temp_dataset\n",
    "from datatools2 import MapDataset, transform_data\n",
    "from datatools3 import get_pr_image\n",
    "from losstools import loss_function_wrapper\n",
    "from models import Hourglass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Declaring tunable hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "# dataset generation hyperparameters\n",
    "patch_size = 200\n",
    "sigma = 9\n",
    "\n",
    "# training hyperparameters\n",
    "epochs = 2.5e3\n",
    "lr = 5e-4\n",
    "batch_size = 25\n",
    "dropout=0\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Hourglass(depth=1, dropout=dropout).to(device)\n",
    "loss_func = loss_function_wrapper('gcel')\n",
    "transform = A.Compose([\n",
    "    A.RandomRotate90(p=1),\n",
    "    A.Transpose(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "])\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# performance logger parameters\n",
    "write = False\n",
    "save_model = False\n",
    "if write or save_model:\n",
    "    folder = './active/2023.09.08 full_size'\n",
    "    model_name = 'full_size, gcel'.format(dropout)\n",
    "if write:\n",
    "    writer = SummaryWriter('./{}/runs/{}'.format(folder, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Constructing a pipeline to control how data is used by the model\n",
    "    - Temporary dataset creation: split dataset's patches into training and validation groups\n",
    "        * use 20190822...11.48.51 AM patch_[range(5, 76, 5)].jpg for training\n",
    "        * use 20190822...11.48.51 AM patch_[80].jpg for validation\n",
    "    - Dataloader creation: package lists of patches into torch Dataloaders\n",
    "\"\"\"\n",
    "\n",
    "# Temporary dataset creation\n",
    "parent_dir = make_dataset('./dataset',\n",
    "                         sigma=sigma,\n",
    "                         patch_size=patch_size,\n",
    "                         full_size=True)\n",
    "exclude_list = ['20190822_movie_01_SampleOldA1_120kV_81x2048x2048_30sec_Aligned 11.48.51 AM patch_080.jpg']\n",
    "train_ds, val_ds = load_temp_dataset(parent_dir, exclude_list)\n",
    "\n",
    "# Dataloader creation\n",
    "train_ds = MapDataset(train_ds)\n",
    "val_ds = MapDataset(val_ds)\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, shuffle=True, batch_size=batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                    | 0/2500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (1995,2022,1) into shape (2022,1995,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m total_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# transform data and send to device\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/merons_unet/./modules/datatools2.py:65\u001b[0m, in \u001b[0;36mtransform_data\u001b[0;34m(images, masks, transform)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_index, image, mask \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), images, masks):\n\u001b[1;32m     64\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m transform(image\u001b[38;5;241m=\u001b[39mimage, mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[0;32m---> 65\u001b[0m     images[batch_index] \u001b[38;5;241m=\u001b[39m transformed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[1;32m     66\u001b[0m     masks[batch_index] \u001b[38;5;241m=\u001b[39m transformed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# ensure that data is of correct type and shape\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (1995,2022,1) into shape (2022,1995,1)"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(int(epochs))):\n",
    "    \"\"\"Training\"\"\"\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        # transform data and send to device\n",
    "        x, y = transform_data(x, y, transform)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        # make prediction and calculate loss\n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        total_train_loss += loss\n",
    "        # backpropogration\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        \"\"\"Validation\"\"\"\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        for x, y in val_loader:\n",
    "            # transform data and send to device\n",
    "            x, y = transform_data(x, y, transform)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            # make prediction and calculate loss\n",
    "            pred = model(x)\n",
    "            loss = loss_func(pred, y)\n",
    "            total_val_loss += loss\n",
    "            \n",
    "    # calculate precision and recall\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    \n",
    "    for image_name in exclude_list:\n",
    "        precision_recall = get_pr_image(parent_dir, image_name, model, patch_size, device)\n",
    "        total_precision += precision_recall[0]\n",
    "        total_recall += precision_recall[1]\n",
    "        \n",
    "    average_precision = total_precision / len(exclude_list)\n",
    "    average_recall = total_recall / len(exclude_list)\n",
    "\n",
    "    # calculate average loss from aggregate\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    # record performance statistics and save model, if necessary\n",
    "    if write:\n",
    "        writer.add_scalar('train_loss', avg_train_loss, epoch)\n",
    "        writer.add_scalar('val_loss', avg_val_loss, epoch)\n",
    "        writer.add_scalar('average_precision', average_precision, epoch)\n",
    "        writer.add_scalar('average_recall', average_recall, epoch)\n",
    "    if save_model:\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            model_param_path = './{}/model_saves/{}, epoch={:05}.pth'.format(folder, model_name, epoch + 1)\n",
    "            torch.save(model.state_dict(), model_param_path)\n",
    "\n",
    "# close performance logger, if necessary\n",
    "if write:\n",
    "    writer.flush()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# for image_name in sorted(os.listdir(os.path.join('./temp_datasets/ds_2', 'images'))):\n",
    "#     if image_name[-4:] in ['.jpg', '.png', 'json']:\n",
    "#         shutil.copyfile(os.path.join(parent_dir, 'images', image_name),\n",
    "#                         os.path.join(parent_dir, 'image_patches', image_name))\n",
    "#         shutil.copyfile(os.path.join(parent_dir, 'masks', image_name),\n",
    "#                         os.path.join(parent_dir, 'mask_patches', image_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./temp_datasets/ds_5/image_patches/20190822_movie_01_SampleOldA1_120kV_81x2048x2048_30sec_Aligned 11.48.51 AM patch_000.jpg/20190822_movie_01_SampleOldA1_120kV_81x2048x2048_30sec_Aligned 11.48.51 AM patch_000.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "Image.open('./temp_datasets/ds_5/image_patches/20190822_movie_01_SampleOldA1_120kV_81x2048x2048_30sec_Aligned 11.48.51 AM patch_000.jpg/20190822_movie_01_SampleOldA1_120kV_81x2048x2048_30sec_Aligned 11.48.51 AM patch_000.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merons",
   "language": "python",
   "name": "merons"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
